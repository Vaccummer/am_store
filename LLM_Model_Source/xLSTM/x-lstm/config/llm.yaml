seed_everything: 31415

model:
  vocab_size: 50257
  num_layers: 8
  signature: [7, 1]
  inp_dim: 64
  head_dim: 16
  head_num: 4
  p_factor: [2, 1.33333] # (2, 4/3)
  ker_size: 4
  tokenizer:
    class_path: xlstm.utils.TokenizerWrapper
    init_args:
      pretrained_model_name_or_path: openai-community/gpt2
      special_tokens:
        pad_token: <|pad|>

  # Parameters relevant for the model inference
  inference_kw:
    prompt: [Once upon a time, In a galaxy far far away]
    token_lim: 128
    use_top_k: 50
    temperature: 1.

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
    weight_decay: 0.01

data:
  root: /path/to/data/root
  read_chunk: 4096
  batch_size: 64
  num_workers: 4 
  tokenizer:
    class_path: xlstm.utils.TokenizerWrapper
    init_args:
      pretrained_model_name_or_path: openai-community/gpt2
      special_tokens:
        pad_token: <|pad|>

trainer:
  max_epochs: 40
  accelerator: gpu
  devices: 1
  strategy: ddp_find_unused_parameters_false
  precision: 16-mixed
  log_every_n_steps: 1
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        save_last: true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: /path/to/save/dir
        name: llm-xlstm
        version: null
